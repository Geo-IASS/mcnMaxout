function net = cnn_cifar_init_mnin(opts)

  data = Input('data') ; labels = Input('labels') ;
  u = [192 160 96] ; p = [1 5 5] ; ker = [5 5] ; poolKer = [3 3] ;
  m1 = add_maxout_block(data, opts, u, p, ker, poolKer) ;
  u = [192 192 192] ; p = [1 5 5] ; ker = [5 5] ; poolKer = [3 3] ;
  m2 = add_maxout_block(m1, opts, u, p, ker, poolKer) ;
  u = [192 192 10] ; p = [1 5 5] ; ker = [3 3] ; poolKer = [8 8] ;
  m3 = add_maxout_block(m2, opts, u, p, ker, poolKer) ;
  loss = Layer.create(@vl_nnloss, {m3, labels}) ;
  net = Net(loss) ;
  net.meta.inputSize = [32 32 3] ;

% -------------------------------------------------------------
function block = add_maxout_block(in, opts, u, p, ker, poolKer) 
% -------------------------------------------------------------
  c1 = add_block(in, opts, [ker(1:2), 3, u(1)*p(1)], 0, 'stride', 1, 'pad', 2) ;
  c2 = add_block(c1, opts, [1, 1, u(1)*p(1), u(2)*p(2)], 0, 'stride', 1, 'pad', 2) ;
  m1 = Layer.create(@vl_maxout, {c2, 'units', u(2), 'pieces', p(2)}) ;
  c3 = add_block(m1, opts, [1, 1, u(2)*p(2), u(3)*p(3)], 0, 'stride', 1, 'pad', 0) ;
  m2 = Layer.create(@vl_maxout, {c3, 'units', u(3), 'pieces', p(3)}) ;
  p1 = vl_nnpool(m2, poolKer, 'method', 'avg', 'stride', 2, 'pad', [0 1 0 1]) ;
  block = vl_nndropout(p1, 'rate', 0.5) ;

% -------------------------------------------------------------
function net = add_block(net, opts, sz, nonLinearity, varargin)
% -------------------------------------------------------------
  filters = Param('value', orthoInit(sz, 'single'), 'learningRate', 1) ;
  biases = Param('value', zeros(sz(4), 1, 'single'), 'learningRate', 2) ;

  net = vl_nnconv(net, filters, biases, varargin{:}) ;

  if nonLinearity
    bn = opts.modelOpts.batchNormalization ;
    rn = opts.modelOpts.batchRenormalization ;
    assert(bn + rn < 2, 'cannot add both batch norm and renorm') ;
    if bn
      net = vl_nnbnorm(net, 'learningRate', [2 1 0.05], 'testMode', false) ;
    elseif rn
      net = vl_nnbrenorm_auto(net, opts.clips, opts.renormLR{:}) ; 
    end
    net = vl_nnrelu(net) ;
  end

% ------------------------------
function x = orthoInit(sz, type)
% ------------------------------
% ORTHOINIT(SZ, TYPE) - orthoInital weight initialisation
%   X = ORTHOINIT(SZ) initialises a tensor of weights X
%   with dimensions SZ such that the fourth dimension of X
%   is orthonormal to the others
%
% NOTES:
%   This form of initialisiation is introduced in the paper:
%   "Exact solutions to the nonlinear dynamics of learning in 
%   deep linear neural networks", Saxe et. al, 2013
%   https://arxiv.org/abs/1312.6120

    initw = randn(prod(sz(1:3)), sz(4), type) ;
    [U,~,V] = svd(initw,'econ') ;
    if numel(V) == prod(sz), src = V ; else, src = U ; end
    x = reshape(src, sz(1), sz(2), sz(3), sz(4)) ;

% base model

  %w = {single(orthoInit(5,5,3,unit0)), b*ones(1,unit0*piece0,'single')} ;
  %net.layers{end+1} = struct('type', 'conv', ...
                %'name', 'conv1', ...
                %'weights', {w}, ...  %32
                %'stride', 1, ...
                %'learningRate', [.1 1], ...
                %'weightDecay', [1 0], ...
                %'pad', 2) ;

  %w = {ones(unit0*piece0, 1, 'single'), zeros(unit0*piece0, 1, 'single')} ;
  %net.layers{end+1} = struct('type', 'bnorm', 'name', 'bn1', ...
            %'weights', {w},'learningRate', [1 1 .5],'weightDecay', [0 0]) ; 
                         
  %% maxout layer 1
  %unit1=160 ; piece1 = 5;
  %w = {single(orthoInit(1,1,unit0,unit1*piece1)), b*ones(1,unit1*piece1,'single')} ;
  %net.layers{end+1} = struct('type', 'conv', ...
                             %'name', 'maxoutconv1', ...
                             %'weights', {w}, ...
                             %'stride', 1, ...
                             %'learningRate', [.1 1], ...
                             %'weightDecay', [1 0], ...
                             %'pad', 0) ;
                         
  %w = {ones(unit1*piece1, 1, 'single'), zeros(unit1*piece1, 1, 'single')} ;
  %net.layers{end+1} = struct('type', 'bnorm', 'name', 'bn2', ...
            %'weights', {w}, 'learningRate', [1 1 .5],'weightDecay', [0 0]) ;   
                         
  %net.layers{end+1} = struct('type', 'maxout','numunit',unit1,'numpiece',piece1) ; 

  %% maxout layer 2
  %unit2 = 96 ; piece2 = 5 ;
  %w = {single(orthoInit(1,1,unit1,unit2*piece2)), b*ones(1,unit2*piece2,'single')} ;
  %net.layers{end+1} = struct('type', 'conv', ...
                             %'name', 'maxoutconv2', ...
                             %'weights', {w}, ...
                             %'stride', 1, ...
                             %'learningRate', [.1 1], ...
                             %'weightDecay', [1 0], ...
                             %'pad', 0) ;

  %w = {ones(unit2*piece2, 1, 'single'), zeros(unit2*piece2, 1, 'single')} ;
  %net.layers{end+1} = struct('type', 'bnorm', 'name', 'bn3', ...
          %'weights', {w},'learningRate', [1 1 .5],'weightDecay', [0 0]) ;   
                         
  %net.layers{end+1} = struct('type', 'maxout','numunit',unit2,'numpiece',piece2) ; 
  %net.layers{end+1} = struct('name', 'pool1', ... %15
                             %'type', 'pool', ...
                             %'method', 'avg', ...
                             %'pool', [3 3], ...
                             %'stride', 2, ...
                             %'pad', [0 1 0 1]) ;
  %net.layers{end+1} = struct('type', 'dropout', 'name', 'dropout1', 'rate', 0.5) ;

  % Block 2
  %unit0 = 192 ; piece0 = 1 ;
  %w = {single(orthoInit(5,5,unit2,unit0)), b*ones(1,unit0*piece0,'single')} ;
  %net.layers{end+1} = struct('type', 'conv', ...
                             %'name', 'conv2', ...
                             %'weights', {w}, ... %16
                             %'stride', 1, ...
                             %'learningRate', [0.1 1], ...
                             %'weightDecay', [1 0], ...
                             %'pad', 2) ;
   %w = {ones(unit0*piece0, 1, 'single'), zeros(unit0*piece0, 1, 'single')} ;
   %net.layers{end+1} = struct('type', 'bnorm', 'name', 'bn4', ...
              %'weights', {w},'learningRate', [1 1 .5],'weightDecay', [0 0]) ; 
                         
  %unit1 = 192 ; piece1 = 5 ;
  %w = {single(orthoInit(1,1,unit0,unit1*piece1)), b*ones(1,unit1*piece1,'single')} ;
  %net.layers{end+1} = struct('type', 'conv', ...
                             %'name', 'maxoutconv3', ...
                             %'weights', {w}, ...
                             %'stride', 1, ...
                             %'learningRate', [.1 1], ...
                             %'weightDecay', [1 0], ...
                             %'pad', 0) ;
  %w = {ones(unit1*piece1, 1, 'single'), zeros(unit1*piece1, 1, 'single')} ;
  %net.layers{end+1} = struct('type', 'bnorm', 'name', 'bn5', ...
           %'weights', {w}, 'learningRate', [1 1 .5],'weightDecay', [0 0]) ;                          
  %net.layers{end+1} = struct('type', 'maxout','numunit',unit1,'numpiece',piece1) ; 

  %unit2 = 192 ; piece2 = 5 ;
  %w = {single(orthoInit(1,1,unit1,unit2*piece2)), b*ones(1,unit2*piece2,'single')} ;
  %net.layers{end+1} = struct('type', 'conv', ...
                             %'name', 'maxoutconv4', ...
                             %'weights', {w}, ...
                             %'stride', 1, ...
                             %'learningRate', [.1 1], ...
                             %'weightDecay', [1 0], ...
                             %'pad', 0) ;

  %w = {ones(unit2*piece2, 1, 'single'), zeros(unit2*piece2, 1, 'single')} ;
  %net.layers{end+1} = struct('type', 'bnorm', 'name', 'bn6', ...
                %'weights', {w},'learningRate', [1 1 .5],'weightDecay', [0 0]) ;                          
  %net.layers{end+1} = struct('type', 'maxout','numunit',unit2,'numpiece',piece2) ; 
  %net.layers{end+1} = struct('name', 'pool2', ... %7
                             %'type', 'pool', ...
                             %'method', 'avg', ...
                             %'pool', [3 3], ...
                             %'stride', 2, ...
                             %'pad', [0 1 0 1]) ;
  %net.layers{end+1} = struct('type', 'dropout', 'name', 'dropout2', 'rate', 0.5) ;

  % Block 3
  %unit0 = 192 ; piece0 = 1 ;
  %w = {single(orthoInit(3,3,unit2,unit0)), b*ones(1, unit0*piece0, 'single')} ;
  %net.layers{end+1} = struct('type', 'conv', ...
                             %'name', 'conv3', ...
                             %'weights', {w}, ... %4
                             %'stride', 1, ...
                             %'learningRate', [.1 1], ...
                             %'weightDecay', [1 0], ...
                             %'pad', 1) ;
   %w =  {ones(unit0*piece0, 1, 'single'), zeros(unit0*piece0, 1, 'single')} ;
   %net.layers{end+1} = struct('type', 'bnorm', 'name', 'bn7', ...
                             %'weights', {w},'learningRate', [1 1 .5],'weightDecay', [0 0]) ; 
  %%% maxout layer 1
  %unit1 = 192 ; piece1 = 5 ;
  %w = {single(orthoInit(1,1,unit0,unit1*piece1)), b*ones(1,unit1*piece1,'single')} ;
  %net.layers{end+1} = struct('type', 'conv', ...
                             %'name', 'maxoutconv5', ...
                             %'weights', {w}, ...
                             %'stride', 1, ...
                             %'learningRate', [.1 1], ...
                             %'weightDecay', [1 0], ...
                             %'pad', 0) ;
  %w = {ones(unit1*piece1, 1, 'single'), zeros(unit1*piece1, 1, 'single')} ;
  %net.layers{end+1} = struct('type', 'bnorm', 'name', 'bn8', ...
         %'weights', {w},'learningRate', [1 1 .5],'weightDecay', [0 0]) ; 
                         
  %net.layers{end+1} = struct('type', 'maxout','numunit',unit1,'numpiece',piece1) ; 

  %unit2 = 10 ; piece2 = 5 ;
  %w = {single(orthoInit(1,1,unit1,unit2*piece2)), b*ones(1,unit2*piece2,'single')} ;
  %net.layers{end+1} = struct('type', 'conv', ...
                             %'name', 'maxoutconv6', ...
                             %'weights', {w}, ...
                             %'stride', 1, ...
                             %'learningRate', 0.1* [.1 1], ...
                             %'weightDecay', [1 0], ...
                             %'pad', 0) ;
  %w = {ones(unit2*piece2, 1, 'single'), zeros(unit2*piece2, 1, 'single')} ;
  %net.layers{end+1} = struct('type', 'bnorm', 'name', 'bn9', ...
               %'weights', {w},'learningRate', [1 1 .5],'weightDecay', [0 0]) ;
                         
  %net.layers{end+1} = struct('type', 'maxout','numunit',unit2,'numpiece',piece2) ;                        
  %net.layers{end+1} = struct('type', 'pool', ...
                             %'name', 'pool3', ...
                             %'method', 'avg', ...
                             %'pool', [8 8] ,...
                             %'stride',1, ...
                             %'pad', 0) ;

                         
  % Loss layer
  %net.layers{end+1} = struct('type', 'softmaxloss') ;

  %% Fill in default values
  %net = vl_simplenn_tidy(net) ;
  %vl_simplenn_display(net,'inputSize', [32 32 3 100]) ;

  %% Meta parameters
  %net.meta.inputSize = [32 32 3] ;
  %net.meta.trainOpts.learningRate = [0.5*ones(1,80) 0.5:-0.005:0.005 0.005:-0.0001:0.00005];
  %net.meta.trainOpts.weightDecay = 0.0005;
  %net.meta.trainOpts.batchSize = 100 ;
  %net.meta.trainOpts.numEpochs = 222;
  %net.meta.trainOpts.augmentation = false;

  %net = dagnn.DagNN.fromSimpleNN(net, 'canonicalNames', true) ;
  %net.addLayer('error', dagnn.Loss('loss', 'classerror'), ...
    %{'prediction','label'}, 'error') ;
  %net = Layer.fromDagNN(net) ;

